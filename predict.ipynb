{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcaf82c",
   "metadata": {},
   "source": [
    "# Human Activity Recognition - Prediction Script\n",
    "\n",
    "This notebook loads the trained LSTM model and predicts activities from new sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c563e22",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322193c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "‚úì TensorFlow loaded in compatibility mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy import signal\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from collections import Counter\n",
    "\n",
    "# TensorFlow 1.x compatibility\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "print(\"‚úì TensorFlow loaded in compatibility mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bf028",
   "metadata": {},
   "source": [
    "# Load Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b79a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  - Input: 128 timesteps √ó 9 features\n",
      "  - Hidden: 32 units\n",
      "  - Classes: 6\n",
      "  - Training accuracy: 91.18%\n"
     ]
    }
   ],
   "source": [
    "# Load model metadata saved during training\n",
    "model_path = './model/lstm_model_v002'\n",
    "\n",
    "with open(f'{model_path}/model_info.json', 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "n_hidden = model_info['n_hidden']\n",
    "n_classes = model_info['n_classes']\n",
    "n_steps = model_info['n_steps']\n",
    "n_input = model_info['n_input']\n",
    "\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  - Input: {n_steps} timesteps √ó {n_input} features\")\n",
    "print(f\"  - Hidden: {n_hidden} units\")\n",
    "print(f\"  - Classes: {n_classes}\")\n",
    "print(f\"  - Training accuracy: {model_info['final_accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd85771",
   "metadata": {},
   "source": [
    "# Rebuild Model Architecture (MUST match training exactly!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd01072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /var/folders/fg/tjm7ykt164500rbcwy6rqv080000gn/T/ipykernel_43493/3192091158.py:20: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:988: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:910: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1702: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model architecture created\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CRITICAL: Clear any previous graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    LSTM neural network - must be IDENTICAL to training code\n",
    "    \"\"\"\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])\n",
    "    \n",
    "    # ReLU activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    _X = tf.split(_X, n_steps, 0)\n",
    "    \n",
    "    # Two stacked LSTM cells\n",
    "    lstm_cell_1 = rnn_cell_impl.LSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_cell_2 = rnn_cell_impl.LSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_cells = rnn_cell_impl.MultiRNNCell([lstm_cell_1, lstm_cell_2])\n",
    "    \n",
    "    outputs, states = tf.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "# Create placeholders and variables - NAMES MUST MATCH TRAINING!\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input], name='input_x')\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden]), name='weights_hidden'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0), name='weights_out')\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden]), name='biases_hidden'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='biases_out')\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "print(\"‚úì Model architecture created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674ed09",
   "metadata": {},
   "source": [
    "# Sensor Data Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132f4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Preprocessing function defined (100Hz ‚Üí 50Hz downsampling)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sensor_data(csv_file, window_size=128, overlap=0.5, input_hz=100, target_hz=50):\n",
    "    \"\"\"\n",
    "    Preprocess raw sensor data from CSV into model-ready format.\n",
    "    \n",
    "    The UCI HAR dataset uses 9 features per timestep at 50 Hz:\n",
    "    - body_acc_x, body_acc_y, body_acc_z (body acceleration, gravity removed)\n",
    "    - body_gyro_x, body_gyro_y, body_gyro_z (angular velocity)\n",
    "    - total_acc_x, total_acc_y, total_acc_z (raw accelerometer)\n",
    "    \n",
    "    IMPORTANT: Phone orientation must match UCI HAR - X-axis pointing UP when standing\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV with columns: acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z\n",
    "        window_size: Number of timesteps per window (default 128, same as UCI HAR)\n",
    "        overlap: Overlap between windows (default 0.5 = 50%)\n",
    "        input_hz: Sampling rate of your sensor data (default 100 Hz for Sensor Logger at 10ms)\n",
    "        target_hz: Target sampling rate for the model (default 50 Hz for UCI HAR)\n",
    "    \n",
    "    Returns:\n",
    "        windows: Array of shape (num_windows, 128, 9)\n",
    "    \"\"\"\n",
    "    # Read CSV - handle both comma and semicolon delimiters\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, delimiter=',')\n",
    "        if len(df.columns) == 1:\n",
    "            df = pd.read_csv(csv_file, delimiter=';')\n",
    "    except:\n",
    "        df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    print(f\"Loaded {csv_file}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Original shape: {df.shape}\")\n",
    "    \n",
    "    # Expected columns\n",
    "    required_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    \n",
    "    # Check for missing columns\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    # missing = []\n",
    "    # for c in required_cols:\n",
    "    #     if c not in df.columns:\n",
    "    #         missing.append(c)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Handle missing values\n",
    "    df_clean = df[required_cols].dropna()\n",
    "    \n",
    "    # DOWNSAMPLE from input_hz to target_hz \n",
    "    if input_hz != target_hz:\n",
    "        downsample_factor = input_hz // target_hz\n",
    "        df_clean = df_clean.iloc[::downsample_factor].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  ‚ö° Downsampling: {input_hz} Hz -> {target_hz} Hz (taking every {downsample_factor}th sample)\")\n",
    "        print(f\"  Shape after downsampling: {df_clean.shape}\")\n",
    "    \n",
    "    if len(df_clean) < window_size:\n",
    "        raise ValueError(f\"Need at least {window_size} samples, got {len(df_clean)}\")\n",
    "    \n",
    "    # Extract sensor data\n",
    "    acc = df_clean[['acc_x', 'acc_y', 'acc_z']].to_numpy()\n",
    "    gyro = df_clean[['gyro_x', 'gyro_y', 'gyro_z']].to_numpy()\n",
    "\n",
    "    # Apply Butterworth high-pass filter to separate body acceleration from gravity\n",
    "    # UCI HAR uses 0.3 Hz cutoff at 50 Hz sampling rate\n",
    "    b, a = signal.butter(3, 0.3, btype='high', fs=target_hz)\n",
    "    body_acc = np.zeros_like(acc)\n",
    "    for i in range(3):\n",
    "        body_acc[:, i] = signal.filtfilt(b, a, acc[:, i])\n",
    "    \n",
    "    # Total acceleration is the raw accelerometer data\n",
    "    total_acc = acc\n",
    "    \n",
    "    # Combine all 9 features in the same order as UCI HAR:\n",
    "    # [body_acc_x, body_acc_y, body_acc_z, gyro_x, gyro_y, gyro_z, total_acc_x, total_acc_y, total_acc_z]\n",
    "    all_features = np.hstack([body_acc, gyro, total_acc])\n",
    "    \n",
    "    # Create sliding windows\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    windows = []\n",
    "    \n",
    "    for start in range(0, len(all_features) - window_size + 1, step_size):\n",
    "        window = all_features[start:start + window_size]\n",
    "        windows.append(window)\n",
    "    \n",
    "    windows = np.array(windows, dtype=np.float32)\n",
    "    print(f\"  Created {len(windows)} windows of shape (128, 9)\")\n",
    "    print(f\"  Each window = {window_size / target_hz:.2f} seconds of activity\")\n",
    "    \n",
    "    return windows\n",
    "\n",
    "print(\"‚úì Preprocessing function defined (100Hz ‚Üí 50Hz downsampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789289a",
   "metadata": {},
   "source": [
    "# Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f2679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "def predict_activity(csv_file, model_path):\n",
    "    \"\"\"\n",
    "    Load model, preprocess data, and predict activity.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV file with sensor data\n",
    "    \n",
    "    Returns:\n",
    "        activity: Predicted activity label\n",
    "        confidence: Confidence percentage\n",
    "    \"\"\"\n",
    "    # Preprocess the sensor data\n",
    "    windows = preprocess_sensor_data(csv_file)\n",
    "    \n",
    "    # Create saver and restore model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Restore trained weights\n",
    "        #saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "        saver.restore(sess, f\"{model_path}/model.ckpt\")\n",
    "        print(\"‚úì Model restored from checkpoint\")\n",
    "        \n",
    "        # Make predictions for all windows\n",
    "        predictions_raw = sess.run(pred, feed_dict={x: windows})\n",
    "        probabilities = sess.run(tf.nn.softmax(predictions_raw))\n",
    "        predicted_classes = predictions_raw.argmax(axis=1)\n",
    "        \n",
    "        # Count predictions\n",
    "        from collections import Counter\n",
    "        \n",
    "        vote_counts = Counter(predicted_classes)\n",
    "        # Get the most common prediction\n",
    "        most_common_class = vote_counts.most_common(1)[0][0]\n",
    "        vote_count = vote_counts.most_common(1)[0][1]\n",
    "        \n",
    "        # Calculate average confidence for the predicted class\n",
    "        confidences = [probabilities[i][predicted_classes[i]] \n",
    "                      for i in range(len(windows))]\n",
    "        avg_confidence = np.mean(confidences) * 100\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"PREDICTION RESULTS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"\\n  Activity: {LABELS[most_common_class]}\")\n",
    "        print(f\"  Confidence: {avg_confidence:.1f}%\")\n",
    "        print(f\"  Windows: {vote_count}/{len(windows)} voted for this class\")\n",
    "        print(f\"\\n  Vote breakdown:\")\n",
    "        for cls, count in vote_counts.most_common():\n",
    "            pct = count / len(windows) * 100\n",
    "            print(f\"    {LABELS[cls]:20s}: {count:3d} ({pct:.1f}%)\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return LABELS[most_common_class], avg_confidence\n",
    "\n",
    "print(\"‚úì Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff16236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì File exists: ./sensor_data/walking2_data.csv\n",
      "  File size: 948372 bytes\n",
      "\n",
      "  First 5 lines:\n",
      "    0: timestamp;acc_z;acc_y;acc_x;gyro_z;gyro_y;gyro_x\n",
      "    1: 1767708643575376600;0.0420989990234375;0.0620880126953125;0.990997314453125;0.03930852189660072;-0.021207816898822784;0.05567098408937454\n",
      "    2: 1767708643585378000;0.0433197021484375;0.0545501708984375;0.987701416015625;0.04080016165971756;-0.03068946860730648;0.051896754652261734\n",
      "    3: 1767708643595378000;0.0416259765625;0.045318603515625;0.985809326171875;0.03905152902007103;-0.038592930883169174;0.05770430713891983\n",
      "    4: 1767708643605377800;0.0326690673828125;0.044281005859375;0.979400634765625;0.03573775663971901;-0.04701331630349159;0.07068055868148804\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = './sensor_data/walking2_data.csv'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"‚úì File exists: {file_path}\")\n",
    "    \n",
    "    # Check file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"  File size: {file_size} bytes\")\n",
    "    \n",
    "    if file_size == 0:\n",
    "        print(\"  ‚ö†Ô∏è FILE IS EMPTY!\")\n",
    "    else:\n",
    "        # Try to read first few lines\n",
    "        with open(file_path, 'r') as f:\n",
    "            print(\"\\n  First 5 lines:\")\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 5:\n",
    "                    break\n",
    "                print(f\"    {i}: {line.strip()}\")\n",
    "else:\n",
    "    print(f\"‚úó File does NOT exist: {file_path}\")\n",
    "    print(\"\\n  Available files:\")\n",
    "    if os.path.exists('./sensor_data/'):\n",
    "        for filename in os.listdir('./sensor_data/'):\n",
    "            print(f\"    - {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8aee7",
   "metadata": {},
   "source": [
    "# Run Predictions\n",
    "\n",
    "Change the file path below to predict on different sensor data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29745abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./sensor_data/walking2_data.csv\n",
      "  Columns: ['timestamp', 'acc_z', 'acc_y', 'acc_x', 'gyro_z', 'gyro_y', 'gyro_x']\n",
      "  Original shape: (6963, 7)\n",
      "  ‚ö° Downsampling: 100 Hz -> 50 Hz (taking every 2th sample)\n",
      "  Shape after downsampling: (3482, 6)\n",
      "  Created 53 windows of shape (128, 9)\n",
      "  Each window = 2.56 seconds of activity\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model_v002/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768399808.959277 14714536 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model restored from checkpoint\n",
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS\n",
      "==================================================\n",
      "\n",
      "  Activity: WALKING_DOWNSTAIRS\n",
      "  Confidence: 86.5%\n",
      "  Windows: 44/53 voted for this class\n",
      "\n",
      "  Vote breakdown:\n",
      "    WALKING_DOWNSTAIRS  :  44 (83.0%)\n",
      "    WALKING_UPSTAIRS    :   7 (13.2%)\n",
      "    SITTING             :   2 (3.8%)\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 6: Run Prediction on Your Data\n",
    "# ============================================\n",
    "walking = './sensor_data/walking2_data.csv'\n",
    "sitting = './sensor_data/sitting2_data.csv'\n",
    "laying = './sensor_data/laying2_data.csv'\n",
    "standing = './sensor_data/standing2_data.csv'\n",
    "\n",
    "# Uncomment to test other activities:\n",
    "activity, confidence = predict_activity(walking, model_path)\n",
    "#activity, confidence = predict_activity(sitting, model_path)\n",
    "# activity, confidence = predict_activity(laying, model_path)\n",
    "# activity, confidence = predict_activity(standing, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2ca9c",
   "metadata": {},
   "source": [
    "## üì¶ SQLite Database Functions\n",
    "\n",
    "Save predictions and sensor data for later retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f8b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database save function defined\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def save_to_database(csv_file, predicted_activity, confidence, windows, model_version, corrected_label=None):\n",
    "    \"\"\"\n",
    "    Save prediction results and sensor data to SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to original CSV\n",
    "        predicted_activity: Predicted label (e.g., \"WALKING\")\n",
    "        confidence: Confidence percentage\n",
    "        windows: Preprocessed sensor windows (numpy array) - shape (num_windows, 128, 9)\n",
    "        model_version: Model version used (e.g., \"v002\")\n",
    "        corrected_label: User-corrected label if different from prediction (optional)\n",
    "    \n",
    "    Returns:\n",
    "        prediction_id: Database ID for this prediction\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect('har_predictions.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Stores prediction data\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS predictions (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT NOT NULL,\n",
    "            file_path TEXT NOT NULL,\n",
    "            predicted_activity TEXT NOT NULL,\n",
    "            predicted_label INTEGER NOT NULL,\n",
    "            corrected_label INTEGER,\n",
    "            confidence REAL NOT NULL,\n",
    "            model_version TEXT NOT NULL,\n",
    "            num_windows INTEGER NOT NULL,\n",
    "            is_correct BOOLEAN\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # This table stores raw ML training data.\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS training_windows (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            prediction_id INTEGER NOT NULL,\n",
    "            window_index INTEGER NOT NULL,\n",
    "            window_data BLOB NOT NULL,\n",
    "            label INTEGER NOT NULL,\n",
    "            FOREIGN KEY (prediction_id) REFERENCES predictions(id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Convert activity name to label (0-5)\n",
    "    predicted_label = LABELS.index(predicted_activity)\n",
    "    corrected_label_int = corrected_label if corrected_label is not None else None\n",
    "    is_correct = corrected_label is None or (corrected_label == predicted_label)\n",
    "    \n",
    "    # Insert prediction record\n",
    "    cursor.execute('''\n",
    "        INSERT INTO predictions \n",
    "        (timestamp, file_path, predicted_activity, predicted_label, corrected_label, \n",
    "         confidence, model_version, num_windows, is_correct)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        datetime.now().isoformat(),\n",
    "        csv_file,\n",
    "        predicted_activity,\n",
    "        predicted_label,\n",
    "        corrected_label_int,\n",
    "        confidence,\n",
    "        model_version,\n",
    "        len(windows),\n",
    "        is_correct\n",
    "    ))\n",
    "    \n",
    "    prediction_id = cursor.lastrowid\n",
    "    \n",
    "    # Insert sensor windows (use pickle for efficient storage)\n",
    "    label_to_use = corrected_label_int if corrected_label_int is not None else predicted_label\n",
    "    \n",
    "    for i, window in enumerate(windows):\n",
    "        # Serialize numpy array (128, 9) to binary\n",
    "        window_blob = pickle.dumps(window)\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT INTO training_windows (prediction_id, window_index, window_data, label)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (prediction_id, i, window_blob, label_to_use))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"‚úì Saved to database: prediction_id={prediction_id}\")\n",
    "    print(f\"  - Stored {len(windows)} windows\")\n",
    "    print(f\"  - Label: {predicted_label} ({predicted_activity})\")\n",
    "    if corrected_label_int is not None:\n",
    "        print(f\"  - Corrected to: {corrected_label_int} ({LABELS[corrected_label_int]})\")\n",
    "    \n",
    "    return prediction_id\n",
    "\n",
    "print(\"‚úì Database save function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b4c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loader function defined\n"
     ]
    }
   ],
   "source": [
    "def load_training_data(limit=None, filter_correct=False):\n",
    "    \"\"\"\n",
    "    Load all stored predictions from database for retraining.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of windows to load (None = all)\n",
    "        filter_correct: If True, only load user-corrected predictions\n",
    "    \n",
    "    Returns:\n",
    "        X_new: np.array of shape (total_windows, 128, 9) - ready for training!\n",
    "        y_new: np.array of shape (total_windows, 1) - labels (0-5)\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect('har_predictions.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Build query based on filters\n",
    "    query = 'SELECT window_data, label FROM training_windows'\n",
    "    \n",
    "    if filter_correct:\n",
    "        query = '''\n",
    "            SELECT tw.window_data, tw.label \n",
    "            FROM training_windows tw\n",
    "            JOIN predictions p ON tw.prediction_id = p.id\n",
    "            WHERE p.corrected_label IS NOT NULL\n",
    "        '''\n",
    "    \n",
    "    if limit:\n",
    "        query += f' LIMIT {limit}'\n",
    "    \n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    \n",
    "    for window_blob, label in rows:\n",
    "        # Deserialize numpy array\n",
    "        window = pickle.loads(window_blob)\n",
    "        X_new.append(window)\n",
    "        y_new.append(label)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    if len(X_new) == 0:\n",
    "        print(\"‚ö†Ô∏è No training data found in database\")\n",
    "        return None, None\n",
    "    \n",
    "    X_new = np.array(X_new, dtype=np.float32)\n",
    "    y_new = np.array(y_new, dtype=np.int32).reshape(-1, 1)\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(X_new)} windows from database\")\n",
    "    print(f\"  - Shape: {X_new.shape}\")\n",
    "    print(f\"  - Labels shape: {y_new.shape}\")\n",
    "    print(f\"  - Label distribution:\")\n",
    "    \n",
    "    from collections import Counter\n",
    "    label_counts = Counter(y_new.flatten())\n",
    "    for label_idx, count in sorted(label_counts.items()):\n",
    "        print(f\"    {LABELS[label_idx]:20s}: {count:5d} windows\")\n",
    "    \n",
    "    return X_new, y_new\n",
    "\n",
    "print(\"‚úì Data loader function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bea7621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database stats function defined\n"
     ]
    }
   ],
   "source": [
    "def view_database_stats():\n",
    "    \"\"\"\n",
    "    View statistics about stored predictions.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect('har_predictions.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Count predictions\n",
    "    cursor.execute('SELECT COUNT(*) FROM predictions')\n",
    "    num_predictions = cursor.fetchone()[0]\n",
    "    \n",
    "    # Count windows\n",
    "    cursor.execute('SELECT COUNT(*) FROM training_windows')\n",
    "    num_windows = cursor.fetchone()[0]\n",
    "    \n",
    "    # Get predictions by activity\n",
    "    cursor.execute('''\n",
    "        SELECT predicted_activity, COUNT(*) as count \n",
    "        FROM predictions \n",
    "        GROUP BY predicted_activity\n",
    "    ''')\n",
    "    activity_counts = cursor.fetchall()\n",
    "    \n",
    "    # Get corrected predictions\n",
    "    cursor.execute('SELECT COUNT(*) FROM predictions WHERE corrected_label IS NOT NULL')\n",
    "    num_corrected = cursor.fetchone()[0]\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DATABASE STATISTICS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"\\n  Total predictions: {num_predictions}\")\n",
    "    print(f\"  Total windows: {num_windows}\")\n",
    "    print(f\"  User-corrected: {num_corrected}\")\n",
    "    print(f\"\\n  Predictions by activity:\")\n",
    "    for activity, count in activity_counts:\n",
    "        print(f\"    {activity:20s}: {count:3d}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "print(\"‚úì Database stats function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de55232",
   "metadata": {},
   "source": [
    "## üîÑ Prediction with Database Integration\n",
    "\n",
    "Enhanced prediction function that saves results to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05088bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced prediction function defined\n"
     ]
    }
   ],
   "source": [
    "def predict_and_save(csv_file, model_path, save_to_db=True, corrected_label=None):\n",
    "    \"\"\"\n",
    "    Predict activity and optionally save to database for retraining.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV file with sensor data\n",
    "        model_path: Path to model directory (e.g., \"model/lstm_model_v002\")\n",
    "        save_to_db: Whether to save results to database (default: True)\n",
    "        corrected_label: If user corrects prediction, provide correct label (0-5)\n",
    "    \n",
    "    Returns:\n",
    "        activity: Predicted activity label\n",
    "        confidence: Confidence percentage\n",
    "        prediction_id: Database ID (if saved)\n",
    "    \"\"\"\n",
    "    # Preprocess the sensor data\n",
    "    windows = preprocess_sensor_data(csv_file)\n",
    "    \n",
    "    # Create saver and restore model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Restore trained weights\n",
    "        saver.restore(sess, f\"{model_path}/model.ckpt\")\n",
    "        print(\"‚úì Model restored from checkpoint\")\n",
    "        \n",
    "        # Make predictions for all windows\n",
    "        predictions_raw = sess.run(pred, feed_dict={x: windows})\n",
    "        probabilities = sess.run(tf.nn.softmax(predictions_raw))\n",
    "        predicted_classes = predictions_raw.argmax(axis=1)\n",
    "        \n",
    "        # Count predictions\n",
    "        from collections import Counter\n",
    "        \n",
    "        vote_counts = Counter(predicted_classes)\n",
    "        # Get the most common prediction\n",
    "        most_common_class = vote_counts.most_common(1)[0][0]\n",
    "        vote_count = vote_counts.most_common(1)[0][1]\n",
    "        \n",
    "        # Calculate average confidence for the predicted class\n",
    "        confidences = [probabilities[i][predicted_classes[i]] \n",
    "                      for i in range(len(windows))]\n",
    "        avg_confidence = np.mean(confidences) * 100\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"PREDICTION RESULTS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"\\n  Activity: {LABELS[most_common_class]}\")\n",
    "        print(f\"  Confidence: {avg_confidence:.1f}%\")\n",
    "        print(f\"  Windows: {vote_count}/{len(windows)} voted for this class\")\n",
    "        print(f\"\\n  Vote breakdown:\")\n",
    "        for cls, count in vote_counts.most_common():\n",
    "            pct = count / len(windows) * 100\n",
    "            print(f\"    {LABELS[cls]:20s}: {count:3d} ({pct:.1f}%)\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        # Save to database if requested\n",
    "        prediction_id = None\n",
    "        if save_to_db:\n",
    "            model_version = model_path.split('/')[-1]  # e.g., \"lstm_model_v002\"\n",
    "            prediction_id = save_to_database(\n",
    "                csv_file=csv_file,\n",
    "                predicted_activity=LABELS[most_common_class],\n",
    "                confidence=avg_confidence,\n",
    "                windows=windows,\n",
    "                model_version=model_version,\n",
    "                corrected_label=corrected_label\n",
    "            )\n",
    "        \n",
    "        return LABELS[most_common_class], avg_confidence, prediction_id\n",
    "\n",
    "print(\"‚úì Enhanced prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091dcbc",
   "metadata": {},
   "source": [
    "## üß™ Example Usage\n",
    "\n",
    "### Example 1: Predict and Save to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513f7862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./sensor_data/standing2_data.csv\n",
      "  Columns: ['timestamp', 'acc_z', 'acc_y', 'acc_x', 'gyro_z', 'gyro_y', 'gyro_x']\n",
      "  Original shape: (4685, 7)\n",
      "  ‚ö° Downsampling: 100 Hz -> 50 Hz (taking every 2th sample)\n",
      "  Shape after downsampling: (2343, 6)\n",
      "  Created 35 windows of shape (128, 9)\n",
      "  Each window = 2.56 seconds of activity\n",
      "INFO:tensorflow:Restoring parameters from model/lstm_model_v002/model.ckpt\n",
      "‚úì Model restored from checkpoint\n",
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS\n",
      "==================================================\n",
      "\n",
      "  Activity: LAYING\n",
      "  Confidence: 98.2%\n",
      "  Windows: 35/35 voted for this class\n",
      "\n",
      "  Vote breakdown:\n",
      "    LAYING              :  35 (100.0%)\n",
      "==================================================\n",
      "\n",
      "‚úì Saved to database: prediction_id=2\n",
      "  - Stored 35 windows\n",
      "  - Label: 5 (LAYING)\n",
      "\n",
      "Result: LAYING with 98.2% confidence\n",
      "Saved as prediction ID: 2\n"
     ]
    }
   ],
   "source": [
    "walking = './sensor_data/walking2_data.csv'\n",
    "sitting = './sensor_data/sitting2_data.csv'\n",
    "laying = './sensor_data/laying2_data.csv'\n",
    "standing = './sensor_data/standing2_data.csv'\n",
    "\n",
    "# Example 1: Basic prediction with database save\n",
    "activity, confidence, pred_id = predict_and_save(\n",
    "    csv_file= standing,\n",
    "    model_path='model/lstm_model_v002',\n",
    "    save_to_db=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResult: {activity} with {confidence:.1f}% confidence\")\n",
    "print(f\"Saved as prediction ID: {pred_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d02b4a",
   "metadata": {},
   "source": [
    "### Example 2: Predict with User Correction\n",
    "\n",
    "If the prediction is wrong, correct it by providing the right label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./sensor_data/walking2_data.csv\n",
      "  Columns: ['timestamp', 'acc_z', 'acc_y', 'acc_x', 'gyro_z', 'gyro_y', 'gyro_x']\n",
      "  Original shape: (6963, 7)\n",
      "  ‚ö° Downsampling: 100 Hz -> 50 Hz (taking every 2th sample)\n",
      "  Shape after downsampling: (3482, 6)\n",
      "  Created 53 windows of shape (128, 9)\n",
      "  Each window = 2.56 seconds of activity\n",
      "INFO:tensorflow:Restoring parameters from model/lstm_model_v002/model.ckpt\n",
      "‚úì Model restored from checkpoint\n",
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS\n",
      "==================================================\n",
      "\n",
      "  Activity: WALKING_DOWNSTAIRS\n",
      "  Confidence: 86.5%\n",
      "  Windows: 44/53 voted for this class\n",
      "\n",
      "  Vote breakdown:\n",
      "    WALKING_DOWNSTAIRS  :  44 (83.0%)\n",
      "    WALKING_UPSTAIRS    :   7 (13.2%)\n",
      "    SITTING             :   2 (3.8%)\n",
      "==================================================\n",
      "\n",
      "‚úì Saved to database: prediction_id=5\n",
      "  - Stored 53 windows\n",
      "  - Label: 2 (WALKING_DOWNSTAIRS)\n",
      "  - Corrected to: 2 (WALKING_DOWNSTAIRS)\n",
      "\n",
      "Model predicted: WALKING_DOWNSTAIRS\n",
      "User corrected to: SITTING\n",
      "This data is now labeled correctly for retraining!\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Prediction was wrong - user corrects it\n",
    "# Model predicted WALKING, but it was actually SITTING\n",
    "\n",
    "# Label mapping reminder:\n",
    "# 0: WALKING, 1: WALKING_UPSTAIRS, 2: WALKING_DOWNSTAIRS, 3: SITTING, 4: STANDING, 5: LAYING\n",
    "\n",
    "activity, confidence, pred_id = predict_and_save(\n",
    "    csv_file=walking,\n",
    "    model_path='model/lstm_model_v002',\n",
    "    save_to_db=True,\n",
    "    corrected_label=2  # User says it's actually SITTING (label 3)\n",
    ")\n",
    "\n",
    "print(f\"\\nModel predicted: {activity}\")\n",
    "print(f\"User corrected to: {LABELS[2]}\")\n",
    "print(f\"This data is now labeled correctly for retraining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f7a4b",
   "metadata": {},
   "source": [
    "### Example 3: View Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2481aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATABASE STATISTICS\n",
      "==================================================\n",
      "\n",
      "  Total predictions: 5\n",
      "  Total windows: 229\n",
      "  User-corrected: 3\n",
      "\n",
      "  Predictions by activity:\n",
      "    LAYING              :   2\n",
      "    WALKING_DOWNSTAIRS  :   3\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View what's in the database\n",
    "view_database_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df28c67",
   "metadata": {},
   "source": [
    "### Example 4: Load Data for Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "114abece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 229 windows from database\n",
      "  - Shape: (229, 128, 9)\n",
      "  - Labels shape: (229, 1)\n",
      "  - Label distribution:\n",
      "    WALKING             :    53 windows\n",
      "    WALKING_DOWNSTAIRS  :   106 windows\n",
      "    STANDING            :    35 windows\n",
      "    LAYING              :    35 windows\n",
      "\n",
      "‚úì Data is ready for retraining!\n",
      "  Use this in train_model.ipynb:\n",
      "  X_train_combined = np.vstack([X_train, X_new])\n",
      "  y_train_combined = np.vstack([y_train, y_new])\n"
     ]
    }
   ],
   "source": [
    "# Load all stored data (ready for training!)\n",
    "X_new, y_new = load_training_data()\n",
    "\n",
    "# Or load only user-corrected data\n",
    "# X_new, y_new = load_training_data(filter_correct=True)\n",
    "\n",
    "# Or load limited number of windows\n",
    "# X_new, y_new = load_training_data(limit=1000)\n",
    "\n",
    "if X_new is not None:\n",
    "    print(f\"\\n‚úì Data is ready for retraining!\")\n",
    "    print(f\"  Use this in train_model.ipynb:\")\n",
    "    print(f\"  X_train_combined = np.vstack([X_train, X_new])\")\n",
    "    print(f\"  y_train_combined = np.vstack([y_train, y_new])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f936140",
   "metadata": {},
   "source": [
    "\n",
    "### **Database**\n",
    "\n",
    "**predictions table:**\n",
    "- `id`: Auto-increment primary key\n",
    "- `timestamp`: When prediction was made\n",
    "- `file_path`: Original CSV file path\n",
    "- `predicted_activity`: Predicted activity name\n",
    "- `predicted_label`: Predicted label (0-5)\n",
    "- `corrected_label`: User-corrected label (if applicable)\n",
    "- `confidence`: Prediction confidence %\n",
    "- `model_version`: Which model version made prediction\n",
    "- `num_windows`: Number of windows in this prediction\n",
    "- `is_correct`: Whether prediction matches correction\n",
    "\n",
    "**training_windows table:**\n",
    "- `id`: Auto-increment primary key\n",
    "- `prediction_id`: Foreign key to predictions\n",
    "- `window_index`: Index of window in prediction\n",
    "- `window_data`: Serialized numpy array (128, 9)\n",
    "- `label`: Activity label (0-5)\n",
    "\n",
    "### **Label Mapping**\n",
    "- 0: WALKING\n",
    "- 1: WALKING_UPSTAIRS\n",
    "- 2: WALKING_DOWNSTAIRS\n",
    "- 3: SITTING\n",
    "- 4: STANDING\n",
    "- 5: LAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80c722",
   "metadata": {},
   "source": [
    "# DEBUG: Test model with UCI HAR training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632147a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded UCI HAR test data: (2947, 128, 9)\n",
      "Testing model on ORIGINAL training data...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "WALKING             : 5/5 correct\n",
      "WALKING_UPSTAIRS    : 5/5 correct\n",
      "WALKING_DOWNSTAIRS  : 5/5 correct\n",
      "SITTING             : 5/5 correct\n",
      "STANDING            : 4/5 correct\n",
      "LAYING              : 5/5 correct\n",
      "\n",
      "Overall accuracy on 100 test samples: 88.0%\n",
      "\n",
      "‚ö†Ô∏è If accuracy is ~16-20%, the saved model is corrupted!\n",
      "   You need to RETRAIN the model with a fresh kernel.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DEBUG: Test model with UCI HAR training data\n",
    "# If it fails on training data too, the model checkpoint is bad\n",
    "# ============================================\n",
    "\n",
    "# Load actual UCI HAR data that the model was trained on\n",
    "uci_path = \"UCI HAR Dataset/\"\n",
    "X_test_signals = []\n",
    "for sig in [\"body_acc_x_\", \"body_acc_y_\", \"body_acc_z_\", \n",
    "            \"body_gyro_x_\", \"body_gyro_y_\", \"body_gyro_z_\",\n",
    "            \"total_acc_x_\", \"total_acc_y_\", \"total_acc_z_\"]:\n",
    "    data = np.loadtxt(f\"{uci_path}test/Inertial Signals/{sig}test.txt\")\n",
    "    X_test_signals.append(data)\n",
    "\n",
    "X_test = np.transpose(np.array(X_test_signals), (1, 2, 0))\n",
    "y_test = np.loadtxt(uci_path + \"test/y_test.txt\", dtype=int) - 1  # 0-indexed\n",
    "\n",
    "print(f\"Loaded UCI HAR test data: {X_test.shape}\")\n",
    "print(f\"Testing model on ORIGINAL training data...\\n\")\n",
    "\n",
    "# Test with the model\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "    \n",
    "    # Test on 10 samples of each class\n",
    "    for class_idx in range(6):\n",
    "        indices = np.where(y_test == class_idx)[0][:5]\n",
    "        correct = 0\n",
    "        for idx in indices:\n",
    "            sample = X_test[idx:idx+1]\n",
    "            pred_raw = sess.run(pred, feed_dict={x: sample})\n",
    "            pred_class = np.argmax(pred_raw)\n",
    "            if pred_class == class_idx:\n",
    "                correct += 1\n",
    "        print(f\"{LABELS[class_idx]:20s}: {correct}/5 correct\")\n",
    "    \n",
    "    # Overall accuracy on first 100 samples\n",
    "    pred_all = sess.run(pred, feed_dict={x: X_test[:100]})\n",
    "    pred_classes = np.argmax(pred_all, axis=1)\n",
    "    accuracy = np.mean(pred_classes == y_test[:100])\n",
    "    print(f\"\\nOverall accuracy on 100 test samples: {accuracy*100:.1f}%\")\n",
    "    print(f\"\\n‚ö†Ô∏è If accuracy is ~16-20%, the saved model is corrupted!\")\n",
    "    print(f\"   You need to RETRAIN the model with a fresh kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7490a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ONLY on STANDING samples\n",
      "Number of STANDING samples: 532\n",
      "Shape: (532, 128, 9)\n",
      "Expected label (0-indexed): 4 = STANDING\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "Predictions for 20 STANDING samples:\n",
      "----------------------------------------\n",
      "  Sample 1: WALKING              ‚úó\n",
      "  Sample 2: STANDING             ‚úì\n",
      "  Sample 3: STANDING             ‚úì\n",
      "  Sample 4: STANDING             ‚úì\n",
      "  Sample 5: STANDING             ‚úì\n",
      "  Sample 6: STANDING             ‚úì\n",
      "  Sample 7: STANDING             ‚úì\n",
      "  Sample 8: STANDING             ‚úì\n",
      "  Sample 9: STANDING             ‚úì\n",
      "  Sample 10: STANDING             ‚úì\n",
      "  Sample 11: STANDING             ‚úì\n",
      "  Sample 12: STANDING             ‚úì\n",
      "  Sample 13: STANDING             ‚úì\n",
      "  Sample 14: STANDING             ‚úì\n",
      "  Sample 15: STANDING             ‚úì\n",
      "  Sample 16: STANDING             ‚úì\n",
      "  Sample 17: STANDING             ‚úì\n",
      "  Sample 18: STANDING             ‚úì\n",
      "  Sample 19: STANDING             ‚úì\n",
      "  Sample 20: STANDING             ‚úì\n",
      "----------------------------------------\n",
      "\n",
      "Accuracy on STANDING: 19/20 = 95.0%\n",
      "\n",
      "Prediction distribution:\n",
      "  WALKING             : 1 (5.0%)\n",
      "  STANDING            : 19 (95.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST: Predict ONLY on STANDING samples (label 5)\n",
    "# ============================================\n",
    "\n",
    "# Filter for only STANDING samples (label 5 in file = index 4 after -1)\n",
    "standing_indices = np.where(y_test == 4)[0]  # 4 because we did -1 earlier (5-1=4)\n",
    "X_standing = X_test[standing_indices]\n",
    "y_standing = y_test[standing_indices]\n",
    "\n",
    "print(f\"Testing ONLY on STANDING samples\")\n",
    "print(f\"Number of STANDING samples: {len(X_standing)}\")\n",
    "print(f\"Shape: {X_standing.shape}\")\n",
    "print(f\"Expected label (0-indexed): 4 = STANDING\\n\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "    \n",
    "    # Predict on first 20 standing samples\n",
    "    n_samples = min(20, len(X_standing))\n",
    "    pred_all = sess.run(pred, feed_dict={x: X_standing[:n_samples]})\n",
    "    pred_classes = np.argmax(pred_all, axis=1)\n",
    "    \n",
    "    print(f\"Predictions for {n_samples} STANDING samples:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(n_samples):\n",
    "        predicted = LABELS[pred_classes[i]]\n",
    "        correct = \"‚úì\" if pred_classes[i] == 4 else \"‚úó\"\n",
    "        print(f\"  Sample {i+1}: {predicted:20s} {correct}\")\n",
    "    \n",
    "    # Summary\n",
    "    correct_count = np.sum(pred_classes == 4)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nAccuracy on STANDING: {correct_count}/{n_samples} = {correct_count/n_samples*100:.1f}%\")\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    from collections import Counter\n",
    "    counts = Counter(pred_classes)\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    for cls, count in sorted(counts.items()):\n",
    "        print(f\"  {LABELS[cls]:20s}: {count} ({count/n_samples*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a7d16f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARING YOUR DATA vs UCI HAR DATA\n",
      "============================================================\n",
      "\n",
      "üìä YOUR STANDING DATA (first 128 samples after downsampling):\n",
      "------------------------------------------------------------\n",
      "  acc_x:  mean=+1.0008, std=0.0093, range=[+0.971, +1.032]\n",
      "  acc_y:  mean=+0.0130, std=0.0074, range=[-0.006, +0.034]\n",
      "  acc_z:  mean=-0.0185, std=0.0256, range=[-0.060, +0.042]\n",
      "  gyro_x: mean=+0.0155, std=0.0546\n",
      "  gyro_y: mean=-0.0131, std=0.0546\n",
      "  gyro_z: mean=-0.0047, std=0.0257\n",
      "\n",
      "  üéØ GRAVITY detected on: X-axis (value ‚âà 1.00g)\n",
      "\n",
      "üìä UCI HAR STANDING DATA (sample #0)\n",
      "------------------------------------------------------------\n",
      "  total_acc_x: mean=+0.9938, std=0.0195, range=[+0.928, +1.055]\n",
      "  total_acc_y: mean=-0.2675, std=0.0098, range=[-0.293, -0.239]\n",
      "  total_acc_z: mean=+0.1387, std=0.0199, range=[+0.024, +0.173]\n",
      "  gyro_x:      mean=+0.1523, std=0.1031\n",
      "  gyro_y:      mean=-0.0079, std=0.1422\n",
      "  gyro_z:      mean=+0.0457, std=0.0307\n",
      "\n",
      "  üéØ GRAVITY detected on: X-axis (value ‚âà 0.99g)\n",
      "\n",
      "üìä UCI HAR LAYING DATA (for comparison)\n",
      "------------------------------------------------------------\n",
      "  total_acc_x: mean=-0.1776\n",
      "  total_acc_y: mean=+0.7345\n",
      "  total_acc_z: mean=+0.6631\n",
      "\n",
      "  üéØ GRAVITY detected on: Z-axis\n",
      "\n",
      "============================================================\n",
      "üîç ANALYSIS:\n",
      "============================================================\n",
      "  Your phone gravity axis:     X\n",
      "  UCI STANDING gravity axis:   X\n",
      "  UCI LAYING gravity axis:     Z\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DIAGNOSTIC: Compare your sensor data vs UCI HAR data\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARING YOUR DATA vs UCI HAR DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load your standing data\n",
    "df_yours = pd.read_csv('sensor_data/standing2_data.csv', delimiter=';')\n",
    "df_yours = df_yours.iloc[::2]  # Downsample to 50Hz\n",
    "\n",
    "# Load UCI HAR standing data (one sample = 128 timesteps)\n",
    "uci_standing_idx = np.where(y_test == 4)[0][0]  # First STANDING sample\n",
    "uci_standing = X_test[uci_standing_idx]  # Shape: (128, 9)\n",
    "\n",
    "print(\"\\nüìä YOUR STANDING DATA (first 128 samples after downsampling):\")\n",
    "print(\"-\" * 60)\n",
    "your_acc_x = df_yours['acc_x'].values[:128]\n",
    "your_acc_y = df_yours['acc_y'].values[:128]\n",
    "your_acc_z = df_yours['acc_z'].values[:128]\n",
    "your_gyro_x = df_yours['gyro_x'].values[:128]\n",
    "your_gyro_y = df_yours['gyro_y'].values[:128]\n",
    "your_gyro_z = df_yours['gyro_z'].values[:128]\n",
    "\n",
    "print(f\"  acc_x:  mean={your_acc_x.mean():+.4f}, std={your_acc_x.std():.4f}, range=[{your_acc_x.min():+.3f}, {your_acc_x.max():+.3f}]\")\n",
    "print(f\"  acc_y:  mean={your_acc_y.mean():+.4f}, std={your_acc_y.std():.4f}, range=[{your_acc_y.min():+.3f}, {your_acc_y.max():+.3f}]\")\n",
    "print(f\"  acc_z:  mean={your_acc_z.mean():+.4f}, std={your_acc_z.std():.4f}, range=[{your_acc_z.min():+.3f}, {your_acc_z.max():+.3f}]\")\n",
    "print(f\"  gyro_x: mean={your_gyro_x.mean():+.4f}, std={your_gyro_x.std():.4f}\")\n",
    "print(f\"  gyro_y: mean={your_gyro_y.mean():+.4f}, std={your_gyro_y.std():.4f}\")\n",
    "print(f\"  gyro_z: mean={your_gyro_z.mean():+.4f}, std={your_gyro_z.std():.4f}\")\n",
    "\n",
    "# Detect gravity axis\n",
    "gravity_axis = \"X\" if abs(your_acc_x.mean()) > 0.8 else (\"Y\" if abs(your_acc_y.mean()) > 0.8 else \"Z\")\n",
    "print(f\"\\n  üéØ GRAVITY detected on: {gravity_axis}-axis (value ‚âà {[your_acc_x.mean(), your_acc_y.mean(), your_acc_z.mean()][['X','Y','Z'].index(gravity_axis)]:.2f}g)\")\n",
    "\n",
    "print(\"\\nüìä UCI HAR STANDING DATA (sample #{})\".format(uci_standing_idx))\n",
    "print(\"-\" * 60)\n",
    "# UCI order: body_acc_x, body_acc_y, body_acc_z, gyro_x, gyro_y, gyro_z, total_acc_x, total_acc_y, total_acc_z\n",
    "uci_total_acc_x = uci_standing[:, 6]\n",
    "uci_total_acc_y = uci_standing[:, 7]\n",
    "uci_total_acc_z = uci_standing[:, 8]\n",
    "uci_gyro_x = uci_standing[:, 3]\n",
    "uci_gyro_y = uci_standing[:, 4]\n",
    "uci_gyro_z = uci_standing[:, 5]\n",
    "\n",
    "print(f\"  total_acc_x: mean={uci_total_acc_x.mean():+.4f}, std={uci_total_acc_x.std():.4f}, range=[{uci_total_acc_x.min():+.3f}, {uci_total_acc_x.max():+.3f}]\")\n",
    "print(f\"  total_acc_y: mean={uci_total_acc_y.mean():+.4f}, std={uci_total_acc_y.std():.4f}, range=[{uci_total_acc_y.min():+.3f}, {uci_total_acc_y.max():+.3f}]\")\n",
    "print(f\"  total_acc_z: mean={uci_total_acc_z.mean():+.4f}, std={uci_total_acc_z.std():.4f}, range=[{uci_total_acc_z.min():+.3f}, {uci_total_acc_z.max():+.3f}]\")\n",
    "print(f\"  gyro_x:      mean={uci_gyro_x.mean():+.4f}, std={uci_gyro_x.std():.4f}\")\n",
    "print(f\"  gyro_y:      mean={uci_gyro_y.mean():+.4f}, std={uci_gyro_y.std():.4f}\")\n",
    "print(f\"  gyro_z:      mean={uci_gyro_z.mean():+.4f}, std={uci_gyro_z.std():.4f}\")\n",
    "\n",
    "uci_gravity_axis = \"X\" if abs(uci_total_acc_x.mean()) > 0.8 else (\"Y\" if abs(uci_total_acc_y.mean()) > 0.8 else \"Z\")\n",
    "print(f\"\\n  üéØ GRAVITY detected on: {uci_gravity_axis}-axis (value ‚âà {[uci_total_acc_x.mean(), uci_total_acc_y.mean(), uci_total_acc_z.mean()][['X','Y','Z'].index(uci_gravity_axis)]:.2f}g)\")\n",
    "\n",
    "# Now check UCI LAYING data for comparison\n",
    "print(\"\\nüìä UCI HAR LAYING DATA (for comparison)\")\n",
    "print(\"-\" * 60)\n",
    "uci_laying_idx = np.where(y_test == 5)[0][0]  # First LAYING sample\n",
    "uci_laying = X_test[uci_laying_idx]\n",
    "uci_lay_acc_x = uci_laying[:, 6]\n",
    "uci_lay_acc_y = uci_laying[:, 7]\n",
    "uci_lay_acc_z = uci_laying[:, 8]\n",
    "\n",
    "print(f\"  total_acc_x: mean={uci_lay_acc_x.mean():+.4f}\")\n",
    "print(f\"  total_acc_y: mean={uci_lay_acc_y.mean():+.4f}\")\n",
    "print(f\"  total_acc_z: mean={uci_lay_acc_z.mean():+.4f}\")\n",
    "uci_lay_gravity = \"X\" if abs(uci_lay_acc_x.mean()) > 0.8 else (\"Y\" if abs(uci_lay_acc_y.mean()) > 0.8 else \"Z\")\n",
    "print(f\"\\n  üéØ GRAVITY detected on: {uci_lay_gravity}-axis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Your phone gravity axis:     {gravity_axis}\")\n",
    "print(f\"  UCI STANDING gravity axis:   {uci_gravity_axis}\")\n",
    "print(f\"  UCI LAYING gravity axis:     {uci_lay_gravity}\")\n",
    "\n",
    "if gravity_axis == uci_lay_gravity and gravity_axis != uci_gravity_axis:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  YOUR DATA MATCHES UCI LAYING ORIENTATION!\")\n",
    "    print(f\"      This explains why the model predicts LAYING.\")\n",
    "    print(f\"\\n  üí° SOLUTION: Rotate/remap your axes to match UCI STANDING orientation.\")\n",
    "    print(f\"      UCI STANDING has gravity on {uci_gravity_axis}-axis\")\n",
    "    print(f\"      Your data has gravity on {gravity_axis}-axis\")\n",
    "elif gravity_axis != uci_gravity_axis:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  AXIS MISMATCH DETECTED!\")\n",
    "    print(f\"      Need to remap axes to match UCI orientation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742858f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
