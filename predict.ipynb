{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcaf82c",
   "metadata": {},
   "source": [
    "# Human Activity Recognition - Prediction Script\n",
    "\n",
    "This notebook loads the trained LSTM model and predicts activities from new sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c563e22",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322193c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "‚úì TensorFlow loaded in compatibility mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy import signal\n",
    "\n",
    "# TensorFlow 1.x compatibility\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# ‚úÖ CRITICAL: Clear any previous graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(\"‚úì TensorFlow loaded in compatibility mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bf028",
   "metadata": {},
   "source": [
    "# Load Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  - Input: 128 timesteps √ó 9 features\n",
      "  - Hidden: 32 units\n",
      "  - Classes: 6\n",
      "  - Training accuracy: 88.39%\n"
     ]
    }
   ],
   "source": [
    "# Load model metadata saved during training\n",
    "with open('./model/model_info.json', 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "n_hidden = model_info['n_hidden']\n",
    "n_classes = model_info['n_classes']\n",
    "n_steps = model_info['n_steps']\n",
    "n_input = model_info['n_input']\n",
    "\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  - Input: {n_steps} timesteps √ó {n_input} features\")\n",
    "print(f\"  - Hidden: {n_hidden} units\")\n",
    "print(f\"  - Classes: {n_classes}\")\n",
    "print(f\"  - Training accuracy: {model_info['final_accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd85771",
   "metadata": {},
   "source": [
    "# Rebuild Model Architecture (MUST match training exactly!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd01072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /var/folders/fg/tjm7ykt164500rbcwy6rqv080000gn/T/ipykernel_44750/475354820.py:22: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:988: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:910: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1702: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model architecture created\n"
     ]
    }
   ],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    LSTM neural network - must be IDENTICAL to training code\n",
    "    \"\"\"\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])\n",
    "    \n",
    "    # ReLU activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    _X = tf.split(_X, n_steps, 0)\n",
    "    \n",
    "    # Two stacked LSTM cells\n",
    "    from tensorflow.python.ops import rnn_cell_impl\n",
    "    lstm_cell_1 = rnn_cell_impl.LSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_cell_2 = rnn_cell_impl.LSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_cells = rnn_cell_impl.MultiRNNCell([lstm_cell_1, lstm_cell_2])\n",
    "    \n",
    "    outputs, states = tf.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "# Create placeholders and variables - NAMES MUST MATCH TRAINING!\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input], name='input_x')\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden]), name='weights_hidden'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0), name='weights_out')\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden]), name='biases_hidden'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='biases_out')\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "print(\"‚úì Model architecture created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674ed09",
   "metadata": {},
   "source": [
    "# Sensor Data Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Preprocessing function defined (with axis remapping & downsampling)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sensor_data(csv_file, window_size=128, overlap=0.5, input_hz=100, target_hz=50):\n",
    "    \"\"\"\n",
    "    Preprocess raw sensor data from CSV into model-ready format.\n",
    "    \n",
    "    The UCI HAR dataset uses 9 features per timestep at 50 Hz:\n",
    "    - body_acc_x, body_acc_y, body_acc_z (body acceleration, gravity removed)\n",
    "    - body_gyro_x, body_gyro_y, body_gyro_z (angular velocity)\n",
    "    - total_acc_x, total_acc_y, total_acc_z (raw accelerometer)\n",
    "    \n",
    "    IMPORTANT: Phone orientation must match UCI HAR - X-axis pointing UP when standing\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV with columns: acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z\n",
    "        window_size: Number of timesteps per window (default 128, same as UCI HAR)\n",
    "        overlap: Overlap between windows (default 0.5 = 50%)\n",
    "        input_hz: Sampling rate of your sensor data (default 100 Hz for Sensor Logger at 10ms)\n",
    "        target_hz: Target sampling rate for the model (default 50 Hz for UCI HAR)\n",
    "    \n",
    "    Returns:\n",
    "        windows: Array of shape (num_windows, 128, 9)\n",
    "    \"\"\"\n",
    "    # Read CSV - handle both comma and semicolon delimiters\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, delimiter=',')\n",
    "        if len(df.columns) == 1:\n",
    "            df = pd.read_csv(csv_file, delimiter=';')\n",
    "    except:\n",
    "        df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    print(f\"Loaded {csv_file}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Original shape: {df.shape}\")\n",
    "    \n",
    "    # Expected columns\n",
    "    required_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    \n",
    "    # Check for missing columns\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean = df[required_cols].dropna()\n",
    "    \n",
    "    # ============================================\n",
    "    # DOWNSAMPLE from input_hz to target_hz (e.g., 100 Hz -> 50 Hz)\n",
    "    # ============================================\n",
    "    if input_hz != target_hz:\n",
    "        downsample_factor = input_hz // target_hz\n",
    "        print(f\"  ‚ö° Downsampling: {input_hz} Hz -> {target_hz} Hz (taking every {downsample_factor}th sample)\")\n",
    "        df_clean = df_clean.iloc[::downsample_factor].reset_index(drop=True)\n",
    "        print(f\"  Shape after downsampling: {df_clean.shape}\")\n",
    "    \n",
    "    if len(df_clean) < window_size:\n",
    "        raise ValueError(f\"Need at least {window_size} samples, got {len(df_clean)}\")\n",
    "    \n",
    "    # Extract sensor data\n",
    "    acc = df_clean[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    gyro = df_clean[['gyro_x', 'gyro_y', 'gyro_z']].values\n",
    "\n",
    "    # Apply Butterworth high-pass filter to separate body acceleration from gravity\n",
    "    # UCI HAR uses 0.3 Hz cutoff at 50 Hz sampling rate\n",
    "    b, a = signal.butter(3, 0.3, btype='high', fs=target_hz)\n",
    "    body_acc = np.zeros_like(acc)\n",
    "    for i in range(3):\n",
    "        body_acc[:, i] = signal.filtfilt(b, a, acc[:, i])\n",
    "    \n",
    "    # Total acceleration is the raw accelerometer data\n",
    "    total_acc = acc\n",
    "    \n",
    "    # Combine all 9 features in the same order as UCI HAR:\n",
    "    # [body_acc_x, body_acc_y, body_acc_z, gyro_x, gyro_y, gyro_z, total_acc_x, total_acc_y, total_acc_z]\n",
    "    all_features = np.hstack([body_acc, gyro, total_acc])\n",
    "    \n",
    "    # Create sliding windows\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    windows = []\n",
    "    \n",
    "    for start in range(0, len(all_features) - window_size + 1, step_size):\n",
    "        window = all_features[start:start + window_size]\n",
    "        windows.append(window)\n",
    "    \n",
    "    windows = np.array(windows, dtype=np.float32)\n",
    "    print(f\"  Created {len(windows)} windows of shape (128, 9)\")\n",
    "    print(f\"  Each window = {window_size / target_hz:.2f} seconds of activity\")\n",
    "    \n",
    "    return windows\n",
    "\n",
    "print(\"‚úì Preprocessing function defined (100Hz ‚Üí 50Hz downsampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789289a",
   "metadata": {},
   "source": [
    "# Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "def predict_activity(csv_file):\n",
    "    \"\"\"\n",
    "    Load model, preprocess data, and predict activity.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV file with sensor data\n",
    "    \n",
    "    Returns:\n",
    "        activity: Predicted activity label\n",
    "        confidence: Confidence percentage\n",
    "    \"\"\"\n",
    "    # Preprocess the sensor data\n",
    "    windows = preprocess_sensor_data(csv_file)\n",
    "    \n",
    "    # Create saver and restore model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Restore trained weights\n",
    "        saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "        print(\"‚úì Model restored from checkpoint\")\n",
    "        \n",
    "        # Make predictions for all windows\n",
    "        predictions_raw = sess.run(pred, feed_dict={x: windows})\n",
    "        probabilities = sess.run(tf.nn.softmax(predictions_raw))\n",
    "        predicted_classes = predictions_raw.argmax(axis=1)\n",
    "        \n",
    "        # Count predictions\n",
    "        from collections import Counter\n",
    "        vote_counts = Counter(predicted_classes)\n",
    "        \n",
    "        # Get the most common prediction\n",
    "        most_common_class = vote_counts.most_common(1)[0][0]\n",
    "        vote_count = vote_counts.most_common(1)[0][1]\n",
    "        \n",
    "        # Calculate average confidence for the predicted class\n",
    "        confidences = [probabilities[i][predicted_classes[i]] \n",
    "                      for i in range(len(windows))]\n",
    "        avg_confidence = np.mean(confidences) * 100\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"PREDICTION RESULTS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"\\n  Activity: {LABELS[most_common_class]}\")\n",
    "        print(f\"  Confidence: {avg_confidence:.1f}%\")\n",
    "        print(f\"  Windows: {vote_count}/{len(windows)} voted for this class\")\n",
    "        print(f\"\\n  Vote breakdown:\")\n",
    "        for cls, count in vote_counts.most_common():\n",
    "            pct = count / len(windows) * 100\n",
    "            print(f\"    {LABELS[cls]:20s}: {count:3d} ({pct:.1f}%)\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return LABELS[most_common_class], avg_confidence\n",
    "\n",
    "print(\"‚úì Prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fc8f3",
   "metadata": {},
   "source": [
    "# Run Predictions\n",
    "\n",
    "Change the file path below to predict on different sensor data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29745abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sensor_data/walking2_data.csv\n",
      "  Columns: ['timestamp', 'acc_z', 'acc_y', 'acc_x', 'gyro_z', 'gyro_y', 'gyro_x']\n",
      "  Original shape: (6963, 7)\n",
      "  ‚ö° Downsampling: 100 Hz -> 50 Hz (taking every 2th sample)\n",
      "  Shape after downsampling: (3482, 6)\n",
      "  Created 53 windows of shape (128, 9)\n",
      "  Each window = 2.56 seconds of activity\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "‚úì Model restored from checkpoint\n",
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS\n",
      "==================================================\n",
      "\n",
      "  Activity: WALKING_DOWNSTAIRS\n",
      "  Confidence: 98.3%\n",
      "  Windows: 52/53 voted for this class\n",
      "\n",
      "  Vote breakdown:\n",
      "    WALKING_DOWNSTAIRS  :  52 (98.1%)\n",
      "    WALKING             :   1 (1.9%)\n",
      "==================================================\n",
      "\n",
      "Loaded sensor_data/standing2_data.csv\n",
      "  Columns: ['timestamp', 'acc_z', 'acc_y', 'acc_x', 'gyro_z', 'gyro_y', 'gyro_x']\n",
      "  Original shape: (3349, 7)\n",
      "  ‚ö° Downsampling: 100 Hz -> 50 Hz (taking every 2th sample)\n",
      "  Shape after downsampling: (1675, 6)\n",
      "  Created 25 windows of shape (128, 9)\n",
      "  Each window = 2.56 seconds of activity\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "‚úì Model restored from checkpoint\n",
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS\n",
      "==================================================\n",
      "\n",
      "  Activity: SITTING\n",
      "  Confidence: 79.3%\n",
      "  Windows: 25/25 voted for this class\n",
      "\n",
      "  Vote breakdown:\n",
      "    SITTING             :  25 (100.0%)\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 6: Run Prediction on Your Data\n",
    "# ============================================\n",
    "\n",
    "# # Test with walking data\n",
    "# activity, confidence = predict_activity('sensor_data/walking_data2.csv')\n",
    "\n",
    "# Uncomment to test other activities:\n",
    "activity, confidence = predict_activity('sensor_data/walking2_data.csv')\n",
    "activity, confidence = predict_activity('sensor_data/standing2_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80c722",
   "metadata": {},
   "source": [
    "# DEBUG: Test model with UCI HAR training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632147a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded UCI HAR test data: (2947, 128, 9)\n",
      "Testing model on ORIGINAL training data...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "WALKING             : 5/5 correct\n",
      "WALKING_UPSTAIRS    : 5/5 correct\n",
      "WALKING_DOWNSTAIRS  : 5/5 correct\n",
      "SITTING             : 5/5 correct\n",
      "STANDING            : 4/5 correct\n",
      "LAYING              : 5/5 correct\n",
      "\n",
      "Overall accuracy on 100 test samples: 88.0%\n",
      "\n",
      "‚ö†Ô∏è If accuracy is ~16-20%, the saved model is corrupted!\n",
      "   You need to RETRAIN the model with a fresh kernel.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DEBUG: Test model with UCI HAR training data\n",
    "# If it fails on training data too, the model checkpoint is bad\n",
    "# ============================================\n",
    "\n",
    "# Load actual UCI HAR data that the model was trained on\n",
    "uci_path = \"UCI HAR Dataset/\"\n",
    "X_test_signals = []\n",
    "for sig in [\"body_acc_x_\", \"body_acc_y_\", \"body_acc_z_\", \n",
    "            \"body_gyro_x_\", \"body_gyro_y_\", \"body_gyro_z_\",\n",
    "            \"total_acc_x_\", \"total_acc_y_\", \"total_acc_z_\"]:\n",
    "    data = np.loadtxt(f\"{uci_path}test/Inertial Signals/{sig}test.txt\")\n",
    "    X_test_signals.append(data)\n",
    "\n",
    "X_test = np.transpose(np.array(X_test_signals), (1, 2, 0))\n",
    "y_test = np.loadtxt(uci_path + \"test/y_test.txt\", dtype=int) - 1  # 0-indexed\n",
    "\n",
    "print(f\"Loaded UCI HAR test data: {X_test.shape}\")\n",
    "print(f\"Testing model on ORIGINAL training data...\\n\")\n",
    "\n",
    "# Test with the model\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "    \n",
    "    # Test on 10 samples of each class\n",
    "    for class_idx in range(6):\n",
    "        indices = np.where(y_test == class_idx)[0][:5]\n",
    "        correct = 0\n",
    "        for idx in indices:\n",
    "            sample = X_test[idx:idx+1]\n",
    "            pred_raw = sess.run(pred, feed_dict={x: sample})\n",
    "            pred_class = np.argmax(pred_raw)\n",
    "            if pred_class == class_idx:\n",
    "                correct += 1\n",
    "        print(f\"{LABELS[class_idx]:20s}: {correct}/5 correct\")\n",
    "    \n",
    "    # Overall accuracy on first 100 samples\n",
    "    pred_all = sess.run(pred, feed_dict={x: X_test[:100]})\n",
    "    pred_classes = np.argmax(pred_all, axis=1)\n",
    "    accuracy = np.mean(pred_classes == y_test[:100])\n",
    "    print(f\"\\nOverall accuracy on 100 test samples: {accuracy*100:.1f}%\")\n",
    "    print(f\"\\n‚ö†Ô∏è If accuracy is ~16-20%, the saved model is corrupted!\")\n",
    "    print(f\"   You need to RETRAIN the model with a fresh kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7490a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ONLY on STANDING samples\n",
      "Number of STANDING samples: 532\n",
      "Shape: (532, 128, 9)\n",
      "Expected label (0-indexed): 4 = STANDING\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "Predictions for 20 STANDING samples:\n",
      "----------------------------------------\n",
      "  Sample 1: WALKING              ‚úó\n",
      "  Sample 2: STANDING             ‚úì\n",
      "  Sample 3: STANDING             ‚úì\n",
      "  Sample 4: STANDING             ‚úì\n",
      "  Sample 5: STANDING             ‚úì\n",
      "  Sample 6: STANDING             ‚úì\n",
      "  Sample 7: STANDING             ‚úì\n",
      "  Sample 8: STANDING             ‚úì\n",
      "  Sample 9: STANDING             ‚úì\n",
      "  Sample 10: STANDING             ‚úì\n",
      "  Sample 11: STANDING             ‚úì\n",
      "  Sample 12: STANDING             ‚úì\n",
      "  Sample 13: STANDING             ‚úì\n",
      "  Sample 14: STANDING             ‚úì\n",
      "  Sample 15: STANDING             ‚úì\n",
      "  Sample 16: STANDING             ‚úì\n",
      "  Sample 17: STANDING             ‚úì\n",
      "  Sample 18: STANDING             ‚úì\n",
      "  Sample 19: STANDING             ‚úì\n",
      "  Sample 20: STANDING             ‚úì\n",
      "----------------------------------------\n",
      "\n",
      "Accuracy on STANDING: 19/20 = 95.0%\n",
      "\n",
      "Prediction distribution:\n",
      "  WALKING             : 1 (5.0%)\n",
      "  STANDING            : 19 (95.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST: Predict ONLY on STANDING samples (label 5)\n",
    "# ============================================\n",
    "\n",
    "# Filter for only STANDING samples (label 5 in file = index 4 after -1)\n",
    "standing_indices = np.where(y_test == 4)[0]  # 4 because we did -1 earlier (5-1=4)\n",
    "X_standing = X_test[standing_indices]\n",
    "y_standing = y_test[standing_indices]\n",
    "\n",
    "print(f\"Testing ONLY on STANDING samples\")\n",
    "print(f\"Number of STANDING samples: {len(X_standing)}\")\n",
    "print(f\"Shape: {X_standing.shape}\")\n",
    "print(f\"Expected label (0-indexed): 4 = STANDING\\n\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "    \n",
    "    # Predict on first 20 standing samples\n",
    "    n_samples = min(20, len(X_standing))\n",
    "    pred_all = sess.run(pred, feed_dict={x: X_standing[:n_samples]})\n",
    "    pred_classes = np.argmax(pred_all, axis=1)\n",
    "    \n",
    "    print(f\"Predictions for {n_samples} STANDING samples:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(n_samples):\n",
    "        predicted = LABELS[pred_classes[i]]\n",
    "        correct = \"‚úì\" if pred_classes[i] == 4 else \"‚úó\"\n",
    "        print(f\"  Sample {i+1}: {predicted:20s} {correct}\")\n",
    "    \n",
    "    # Summary\n",
    "    correct_count = np.sum(pred_classes == 4)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nAccuracy on STANDING: {correct_count}/{n_samples} = {correct_count/n_samples*100:.1f}%\")\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    from collections import Counter\n",
    "    counts = Counter(pred_classes)\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    for cls, count in sorted(counts.items()):\n",
    "        print(f\"  {LABELS[cls]:20s}: {count} ({count/n_samples*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a7d16f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARING YOUR DATA vs UCI HAR DATA\n",
      "============================================================\n",
      "\n",
      "üìä YOUR STANDING DATA (first 128 samples after downsampling):\n",
      "------------------------------------------------------------\n",
      "  acc_x:  mean=+1.0008, std=0.0093, range=[+0.971, +1.032]\n",
      "  acc_y:  mean=+0.0130, std=0.0074, range=[-0.006, +0.034]\n",
      "  acc_z:  mean=-0.0185, std=0.0256, range=[-0.060, +0.042]\n",
      "  gyro_x: mean=+0.0155, std=0.0546\n",
      "  gyro_y: mean=-0.0131, std=0.0546\n",
      "  gyro_z: mean=-0.0047, std=0.0257\n",
      "\n",
      "  üéØ GRAVITY detected on: X-axis (value ‚âà 1.00g)\n",
      "\n",
      "üìä UCI HAR STANDING DATA (sample #0)\n",
      "------------------------------------------------------------\n",
      "  total_acc_x: mean=+0.9938, std=0.0195, range=[+0.928, +1.055]\n",
      "  total_acc_y: mean=-0.2675, std=0.0098, range=[-0.293, -0.239]\n",
      "  total_acc_z: mean=+0.1387, std=0.0199, range=[+0.024, +0.173]\n",
      "  gyro_x:      mean=+0.1523, std=0.1031\n",
      "  gyro_y:      mean=-0.0079, std=0.1422\n",
      "  gyro_z:      mean=+0.0457, std=0.0307\n",
      "\n",
      "  üéØ GRAVITY detected on: X-axis (value ‚âà 0.99g)\n",
      "\n",
      "üìä UCI HAR LAYING DATA (for comparison)\n",
      "------------------------------------------------------------\n",
      "  total_acc_x: mean=-0.1776\n",
      "  total_acc_y: mean=+0.7345\n",
      "  total_acc_z: mean=+0.6631\n",
      "\n",
      "  üéØ GRAVITY detected on: Z-axis\n",
      "\n",
      "============================================================\n",
      "üîç ANALYSIS:\n",
      "============================================================\n",
      "  Your phone gravity axis:     X\n",
      "  UCI STANDING gravity axis:   X\n",
      "  UCI LAYING gravity axis:     Z\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DIAGNOSTIC: Compare your sensor data vs UCI HAR data\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARING YOUR DATA vs UCI HAR DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load your standing data\n",
    "df_yours = pd.read_csv('sensor_data/standing2_data.csv', delimiter=';')\n",
    "df_yours = df_yours.iloc[::2]  # Downsample to 50Hz\n",
    "\n",
    "# Load UCI HAR standing data (one sample = 128 timesteps)\n",
    "uci_standing_idx = np.where(y_test == 4)[0][0]  # First STANDING sample\n",
    "uci_standing = X_test[uci_standing_idx]  # Shape: (128, 9)\n",
    "\n",
    "print(\"\\nüìä YOUR STANDING DATA (first 128 samples after downsampling):\")\n",
    "print(\"-\" * 60)\n",
    "your_acc_x = df_yours['acc_x'].values[:128]\n",
    "your_acc_y = df_yours['acc_y'].values[:128]\n",
    "your_acc_z = df_yours['acc_z'].values[:128]\n",
    "your_gyro_x = df_yours['gyro_x'].values[:128]\n",
    "your_gyro_y = df_yours['gyro_y'].values[:128]\n",
    "your_gyro_z = df_yours['gyro_z'].values[:128]\n",
    "\n",
    "print(f\"  acc_x:  mean={your_acc_x.mean():+.4f}, std={your_acc_x.std():.4f}, range=[{your_acc_x.min():+.3f}, {your_acc_x.max():+.3f}]\")\n",
    "print(f\"  acc_y:  mean={your_acc_y.mean():+.4f}, std={your_acc_y.std():.4f}, range=[{your_acc_y.min():+.3f}, {your_acc_y.max():+.3f}]\")\n",
    "print(f\"  acc_z:  mean={your_acc_z.mean():+.4f}, std={your_acc_z.std():.4f}, range=[{your_acc_z.min():+.3f}, {your_acc_z.max():+.3f}]\")\n",
    "print(f\"  gyro_x: mean={your_gyro_x.mean():+.4f}, std={your_gyro_x.std():.4f}\")\n",
    "print(f\"  gyro_y: mean={your_gyro_y.mean():+.4f}, std={your_gyro_y.std():.4f}\")\n",
    "print(f\"  gyro_z: mean={your_gyro_z.mean():+.4f}, std={your_gyro_z.std():.4f}\")\n",
    "\n",
    "# Detect gravity axis\n",
    "gravity_axis = \"X\" if abs(your_acc_x.mean()) > 0.8 else (\"Y\" if abs(your_acc_y.mean()) > 0.8 else \"Z\")\n",
    "print(f\"\\n  üéØ GRAVITY detected on: {gravity_axis}-axis (value ‚âà {[your_acc_x.mean(), your_acc_y.mean(), your_acc_z.mean()][['X','Y','Z'].index(gravity_axis)]:.2f}g)\")\n",
    "\n",
    "print(\"\\nüìä UCI HAR STANDING DATA (sample #{})\".format(uci_standing_idx))\n",
    "print(\"-\" * 60)\n",
    "# UCI order: body_acc_x, body_acc_y, body_acc_z, gyro_x, gyro_y, gyro_z, total_acc_x, total_acc_y, total_acc_z\n",
    "uci_total_acc_x = uci_standing[:, 6]\n",
    "uci_total_acc_y = uci_standing[:, 7]\n",
    "uci_total_acc_z = uci_standing[:, 8]\n",
    "uci_gyro_x = uci_standing[:, 3]\n",
    "uci_gyro_y = uci_standing[:, 4]\n",
    "uci_gyro_z = uci_standing[:, 5]\n",
    "\n",
    "print(f\"  total_acc_x: mean={uci_total_acc_x.mean():+.4f}, std={uci_total_acc_x.std():.4f}, range=[{uci_total_acc_x.min():+.3f}, {uci_total_acc_x.max():+.3f}]\")\n",
    "print(f\"  total_acc_y: mean={uci_total_acc_y.mean():+.4f}, std={uci_total_acc_y.std():.4f}, range=[{uci_total_acc_y.min():+.3f}, {uci_total_acc_y.max():+.3f}]\")\n",
    "print(f\"  total_acc_z: mean={uci_total_acc_z.mean():+.4f}, std={uci_total_acc_z.std():.4f}, range=[{uci_total_acc_z.min():+.3f}, {uci_total_acc_z.max():+.3f}]\")\n",
    "print(f\"  gyro_x:      mean={uci_gyro_x.mean():+.4f}, std={uci_gyro_x.std():.4f}\")\n",
    "print(f\"  gyro_y:      mean={uci_gyro_y.mean():+.4f}, std={uci_gyro_y.std():.4f}\")\n",
    "print(f\"  gyro_z:      mean={uci_gyro_z.mean():+.4f}, std={uci_gyro_z.std():.4f}\")\n",
    "\n",
    "uci_gravity_axis = \"X\" if abs(uci_total_acc_x.mean()) > 0.8 else (\"Y\" if abs(uci_total_acc_y.mean()) > 0.8 else \"Z\")\n",
    "print(f\"\\n  üéØ GRAVITY detected on: {uci_gravity_axis}-axis (value ‚âà {[uci_total_acc_x.mean(), uci_total_acc_y.mean(), uci_total_acc_z.mean()][['X','Y','Z'].index(uci_gravity_axis)]:.2f}g)\")\n",
    "\n",
    "# Now check UCI LAYING data for comparison\n",
    "print(\"\\nüìä UCI HAR LAYING DATA (for comparison)\")\n",
    "print(\"-\" * 60)\n",
    "uci_laying_idx = np.where(y_test == 5)[0][0]  # First LAYING sample\n",
    "uci_laying = X_test[uci_laying_idx]\n",
    "uci_lay_acc_x = uci_laying[:, 6]\n",
    "uci_lay_acc_y = uci_laying[:, 7]\n",
    "uci_lay_acc_z = uci_laying[:, 8]\n",
    "\n",
    "print(f\"  total_acc_x: mean={uci_lay_acc_x.mean():+.4f}\")\n",
    "print(f\"  total_acc_y: mean={uci_lay_acc_y.mean():+.4f}\")\n",
    "print(f\"  total_acc_z: mean={uci_lay_acc_z.mean():+.4f}\")\n",
    "uci_lay_gravity = \"X\" if abs(uci_lay_acc_x.mean()) > 0.8 else (\"Y\" if abs(uci_lay_acc_y.mean()) > 0.8 else \"Z\")\n",
    "print(f\"\\n  üéØ GRAVITY detected on: {uci_lay_gravity}-axis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Your phone gravity axis:     {gravity_axis}\")\n",
    "print(f\"  UCI STANDING gravity axis:   {uci_gravity_axis}\")\n",
    "print(f\"  UCI LAYING gravity axis:     {uci_lay_gravity}\")\n",
    "\n",
    "if gravity_axis == uci_lay_gravity and gravity_axis != uci_gravity_axis:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  YOUR DATA MATCHES UCI LAYING ORIENTATION!\")\n",
    "    print(f\"      This explains why the model predicts LAYING.\")\n",
    "    print(f\"\\n  üí° SOLUTION: Rotate/remap your axes to match UCI STANDING orientation.\")\n",
    "    print(f\"      UCI STANDING has gravity on {uci_gravity_axis}-axis\")\n",
    "    print(f\"      Your data has gravity on {gravity_axis}-axis\")\n",
    "elif gravity_axis != uci_gravity_axis:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  AXIS MISMATCH DETECTED!\")\n",
    "    print(f\"      Need to remap axes to match UCI orientation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742858f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
