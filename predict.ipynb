{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcaf82c",
   "metadata": {},
   "source": [
    "# Human Activity Recognition - Prediction Script\n",
    "\n",
    "This notebook loads the trained LSTM model and predicts activities from new sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322193c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorFlow loaded in compatibility mode\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 1: Imports and Setup\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy import signal\n",
    "\n",
    "# TensorFlow 1.x compatibility\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# ✅ CRITICAL: Clear any previous graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(\"✓ TensorFlow loaded in compatibility mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  - Input: 128 timesteps × 9 features\n",
      "  - Hidden: 32 units\n",
      "  - Classes: 6\n",
      "  - Training accuracy: 89.92%\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 2: Load Model Configuration\n",
    "# ============================================\n",
    "\n",
    "# Load model metadata saved during training\n",
    "with open('./model/model_info.json', 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "n_hidden = model_info['n_hidden']\n",
    "n_classes = model_info['n_classes']\n",
    "n_steps = model_info['n_steps']\n",
    "n_input = model_info['n_input']\n",
    "\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  - Input: {n_steps} timesteps × {n_input} features\")\n",
    "print(f\"  - Hidden: {n_hidden} units\")\n",
    "print(f\"  - Classes: {n_classes}\")\n",
    "print(f\"  - Training accuracy: {model_info['final_accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd01072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:910: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/Users/didiermupenda/miniforge3/envs/dl-projects/lib/python3.12/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1702: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model architecture created\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 3: Rebuild Model Architecture (MUST match training exactly!)\n",
    "# ============================================\n",
    "\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    LSTM neural network - must be IDENTICAL to training code\n",
    "    \"\"\"\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])\n",
    "    \n",
    "    # ReLU activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    _X = tf.split(_X, n_steps, 0)\n",
    "    \n",
    "    # Two stacked LSTM cells\n",
    "    from tensorflow.python.ops import rnn_cell_impl\n",
    "    lstm_cell_1 = rnn_cell_impl.LSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_cell_2 = rnn_cell_impl.LSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_cells = rnn_cell_impl.MultiRNNCell([lstm_cell_1, lstm_cell_2])\n",
    "    \n",
    "    outputs, states = tf.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "# Create placeholders and variables - NAMES MUST MATCH TRAINING!\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input], name='input_x')\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden]), name='weights_hidden'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0), name='weights_out')\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden]), name='biases_hidden'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='biases_out')\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "print(\"✓ Model architecture created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 4: Sensor Data Preprocessing Function\n",
    "# ============================================\n",
    "\n",
    "def preprocess_sensor_data(csv_file, window_size=128, overlap=0.5):\n",
    "    \"\"\"\n",
    "    Preprocess raw sensor data from CSV into model-ready format.\n",
    "    \n",
    "    The UCI HAR dataset uses 9 features per timestep:\n",
    "    - body_acc_x, body_acc_y, body_acc_z (body acceleration, gravity removed)\n",
    "    - body_gyro_x, body_gyro_y, body_gyro_z (angular velocity)\n",
    "    - total_acc_x, total_acc_y, total_acc_z (raw accelerometer)\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV with columns: acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z\n",
    "        window_size: Number of timesteps per window (default 128, same as UCI HAR)\n",
    "        overlap: Overlap between windows (default 0.5 = 50%)\n",
    "    \n",
    "    Returns:\n",
    "        windows: Array of shape (num_windows, 128, 9)\n",
    "    \"\"\"\n",
    "    # Read CSV - handle both comma and semicolon delimiters\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, delimiter=',')\n",
    "        if len(df.columns) == 1:\n",
    "            df = pd.read_csv(csv_file, delimiter=';')\n",
    "    except:\n",
    "        df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    print(f\"Loaded {csv_file}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    # Expected columns\n",
    "    required_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    \n",
    "    # Check for missing columns\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean = df[required_cols].dropna()\n",
    "    if len(df_clean) < window_size:\n",
    "        raise ValueError(f\"Need at least {window_size} samples, got {len(df_clean)}\")\n",
    "    \n",
    "    # Extract sensor data\n",
    "    acc = df_clean[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    gyro = df_clean[['gyro_x', 'gyro_y', 'gyro_z']].values\n",
    "    \n",
    "    # Apply Butterworth high-pass filter to separate body acceleration from gravity\n",
    "    # UCI HAR uses 0.3 Hz cutoff at 50 Hz sampling rate\n",
    "    b, a = signal.butter(3, 0.3, btype='high', fs=50)\n",
    "    body_acc = np.zeros_like(acc)\n",
    "    for i in range(3):\n",
    "        body_acc[:, i] = signal.filtfilt(b, a, acc[:, i])\n",
    "    \n",
    "    # Total acceleration is the raw accelerometer data\n",
    "    total_acc = acc\n",
    "    \n",
    "    # Combine all 9 features in the same order as UCI HAR:\n",
    "    # [body_acc_x, body_acc_y, body_acc_z, gyro_x, gyro_y, gyro_z, total_acc_x, total_acc_y, total_acc_z]\n",
    "    all_features = np.hstack([body_acc, gyro, total_acc])\n",
    "    \n",
    "    # Create sliding windows\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    windows = []\n",
    "    \n",
    "    for start in range(0, len(all_features) - window_size + 1, step_size):\n",
    "        window = all_features[start:start + window_size]\n",
    "        windows.append(window)\n",
    "    \n",
    "    windows = np.array(windows, dtype=np.float32)\n",
    "    print(f\"  Created {len(windows)} windows of shape (128, 9)\")\n",
    "    \n",
    "    return windows\n",
    "\n",
    "print(\"✓ Preprocessing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 5: Prediction Function\n",
    "# ============================================\n",
    "\n",
    "def predict_activity(csv_file):\n",
    "    \"\"\"\n",
    "    Load model, preprocess data, and predict activity.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV file with sensor data\n",
    "    \n",
    "    Returns:\n",
    "        activity: Predicted activity label\n",
    "        confidence: Confidence percentage\n",
    "    \"\"\"\n",
    "    # Preprocess the sensor data\n",
    "    windows = preprocess_sensor_data(csv_file)\n",
    "    \n",
    "    # Create saver and restore model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Restore trained weights\n",
    "        saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "        print(\"✓ Model restored from checkpoint\")\n",
    "        \n",
    "        # Make predictions for all windows\n",
    "        predictions_raw = sess.run(pred, feed_dict={x: windows})\n",
    "        probabilities = sess.run(tf.nn.softmax(predictions_raw))\n",
    "        predicted_classes = predictions_raw.argmax(axis=1)\n",
    "        \n",
    "        # Count predictions\n",
    "        from collections import Counter\n",
    "        vote_counts = Counter(predicted_classes)\n",
    "        \n",
    "        # Get the most common prediction\n",
    "        most_common_class = vote_counts.most_common(1)[0][0]\n",
    "        vote_count = vote_counts.most_common(1)[0][1]\n",
    "        \n",
    "        # Calculate average confidence for the predicted class\n",
    "        confidences = [probabilities[i][predicted_classes[i]] \n",
    "                      for i in range(len(windows))]\n",
    "        avg_confidence = np.mean(confidences) * 100\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"PREDICTION RESULTS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"\\n  Activity: {LABELS[most_common_class]}\")\n",
    "        print(f\"  Confidence: {avg_confidence:.1f}%\")\n",
    "        print(f\"  Windows: {vote_count}/{len(windows)} voted for this class\")\n",
    "        print(f\"\\n  Vote breakdown:\")\n",
    "        for cls, count in vote_counts.most_common():\n",
    "            pct = count / len(windows) * 100\n",
    "            print(f\"    {LABELS[cls]:20s}: {count:3d} ({pct:.1f}%)\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return LABELS[most_common_class], avg_confidence\n",
    "\n",
    "print(\"✓ Prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fc8f3",
   "metadata": {},
   "source": [
    "# Run Predictions\n",
    "\n",
    "Change the file path below to predict on different sensor data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29745abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sensor_data/standing_data.csv\n",
      "  Columns: ['timestamp', 'acc_z', 'acc_y', 'acc_x', 'gyro_z', 'gyro_y', 'gyro_x']\n",
      "  Shape: (4685, 7)\n",
      "  Created 72 windows of shape (128, 9)\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "✓ Model restored from checkpoint\n",
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS\n",
      "==================================================\n",
      "\n",
      "  Activity: LAYING\n",
      "  Confidence: 19.6%\n",
      "  Windows: 72/72 voted for this class\n",
      "\n",
      "  Vote breakdown:\n",
      "    LAYING              :  72 (100.0%)\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 6: Run Prediction on Your Data\n",
    "# ============================================\n",
    "\n",
    "# # Test with walking data\n",
    "# activity, confidence = predict_activity('sensor_data/walking_data2.csv')\n",
    "\n",
    "# Uncomment to test other activities:\n",
    "activity, confidence = predict_activity('sensor_data/standing_data.csv')\n",
    "# activity, confidence = predict_activity('sensor_data/laying_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632147a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded UCI HAR test data: (2947, 128, 9)\n",
      "Testing model on ORIGINAL training data...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/lstm_model.ckpt\n",
      "WALKING             : 0/5 correct\n",
      "WALKING_UPSTAIRS    : 0/5 correct\n",
      "WALKING_DOWNSTAIRS  : 0/5 correct\n",
      "SITTING             : 0/5 correct\n",
      "STANDING            : 0/5 correct\n",
      "LAYING              : 5/5 correct\n",
      "\n",
      "Overall accuracy on 100 test samples: 24.0%\n",
      "\n",
      "⚠️ If accuracy is ~16-20%, the saved model is corrupted!\n",
      "   You need to RETRAIN the model with a fresh kernel.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DEBUG: Test model with UCI HAR training data\n",
    "# If it fails on training data too, the model checkpoint is bad\n",
    "# ============================================\n",
    "\n",
    "# Load actual UCI HAR data that the model was trained on\n",
    "uci_path = \"data/UCI_HAR_Dataset/\"\n",
    "X_test_signals = []\n",
    "for sig in [\"body_acc_x_\", \"body_acc_y_\", \"body_acc_z_\", \n",
    "            \"body_gyro_x_\", \"body_gyro_y_\", \"body_gyro_z_\",\n",
    "            \"total_acc_x_\", \"total_acc_y_\", \"total_acc_z_\"]:\n",
    "    data = np.loadtxt(f\"{uci_path}test/Inertial Signals/{sig}test.txt\")\n",
    "    X_test_signals.append(data)\n",
    "\n",
    "X_test = np.transpose(np.array(X_test_signals), (1, 2, 0))\n",
    "y_test = np.loadtxt(uci_path + \"test/y_test.txt\", dtype=int) - 1  # 0-indexed\n",
    "\n",
    "print(f\"Loaded UCI HAR test data: {X_test.shape}\")\n",
    "print(f\"Testing model on ORIGINAL training data...\\n\")\n",
    "\n",
    "# Test with the model\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/lstm_model.ckpt\")\n",
    "    \n",
    "    # Test on 10 samples of each class\n",
    "    for class_idx in range(6):\n",
    "        indices = np.where(y_test == class_idx)[0][:5]\n",
    "        correct = 0\n",
    "        for idx in indices:\n",
    "            sample = X_test[idx:idx+1]\n",
    "            pred_raw = sess.run(pred, feed_dict={x: sample})\n",
    "            pred_class = np.argmax(pred_raw)\n",
    "            if pred_class == class_idx:\n",
    "                correct += 1\n",
    "        print(f\"{LABELS[class_idx]:20s}: {correct}/5 correct\")\n",
    "    \n",
    "    # Overall accuracy on first 100 samples\n",
    "    pred_all = sess.run(pred, feed_dict={x: X_test[:100]})\n",
    "    pred_classes = np.argmax(pred_all, axis=1)\n",
    "    accuracy = np.mean(pred_classes == y_test[:100])\n",
    "    print(f\"\\nOverall accuracy on 100 test samples: {accuracy*100:.1f}%\")\n",
    "    print(f\"\\n⚠️ If accuracy is ~16-20%, the saved model is corrupted!\")\n",
    "    print(f\"   You need to RETRAIN the model with a fresh kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600c135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
